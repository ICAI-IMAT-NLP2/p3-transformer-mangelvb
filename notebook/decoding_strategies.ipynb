{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Different Decoding Strategies with Hugging Face Transformers\n",
    "In this notebook, we will explore various decoding strategies for text generation using a small encoder-decoder model from Hugging Face's Transformers library. We'll apply these strategies to two different datasets:\n",
    "\n",
    "- Translation task where deterministic strategies are expected to perform better.\n",
    "- Summarization task where stochastic strategies might yield more diverse and informative outputs.\n",
    "\n",
    "The decoding strategies we'll test include:\n",
    "\n",
    "1. Greedy Search\n",
    "2. Beam Search\n",
    "3. Temperature Sampling\n",
    "4. Top-k Sampling\n",
    "5. Top-p (Nucleus) Sampling\n",
    "\n",
    "We'll define a custom ```generate_text``` function to apply these strategies and evaluate their performance using appropriate metrics for each dataset.\n",
    "\n",
    "Here are some useful links you might want to check:\n",
    "- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- [transformers\\AutoModel](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)\n",
    "- [transformers\\AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [Google's T5](https://arxiv.org/pdf/1910.10683)\n",
    "- [T5-small](https://huggingface.co/google-t5/t5-small)\n",
    "- [Flan-T5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets sacrebleu rouge_score evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer\n",
    "\n",
    "We'll use the ```flan-T5-small``` model, which is a small encoder-decoder model suitable for both story_gen and summarization tasks. This model is based on Google's ```t5-small``` model, but fine-tuned on more than 1000 additional tasks covering also more languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mange\\Repositorio\\entornos_virtuales\\ai-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, strategy='greedy', max_length=50, **kwargs):\n",
    "    \"\"\"Generates text based on the specified decoding strategy.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be processed by the model.\n",
    "        strategy (str, optional): The decoding strategy to use. Defaults to 'greedy'.\n",
    "            Options include:\n",
    "            - 'greedy': Greedy search decoding.\n",
    "            - 'beam': Beam search decoding.\n",
    "            - 'temperature': Temperature sampling.\n",
    "            - 'top-k': Top-k sampling.\n",
    "            - 'top-p': Top-p (nucleus) sampling.\n",
    "            - 'contrastive': Contrastive search decoding.\n",
    "        max_length (int, optional): The maximum length of the generated text. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments specific to the decoding strategy.\n",
    "\n",
    "    Keyword Args:\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n",
    "            Used when `strategy='beam'`.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            Used when `strategy='temperature'`.\n",
    "        top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k filtering. Defaults to 50.\n",
    "            Used when `strategy='top-k'` or when `strategy='contrastive'`.\n",
    "        top_p (float, optional): Cumulative probability for nucleus sampling. Defaults to 0.95.\n",
    "            Used when `strategy='top-p'`.\n",
    "        penalty_alpha (float, optional): Contrastive search penalty factor. Defaults to 0.6.\n",
    "            Used when `strategy='contrastive'`.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the decoding strategy.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown decoding strategy is specified.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'greedy':\n",
    "            output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        elif strategy == 'beam':\n",
    "            num_beams = kwargs.get('num_beams', 5)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True\n",
    "            )\n",
    "        elif strategy == 'temperature':\n",
    "            temperature = kwargs.get('temperature', 1.0)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, temperature=temperature\n",
    "            )\n",
    "        elif strategy == 'top-k':\n",
    "            top_k = kwargs.get('top_k', 50)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_k=top_k\n",
    "            )\n",
    "        elif strategy == 'top-p':\n",
    "            top_p = kwargs.get('top_p', 0.95)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_p=top_p\n",
    "            )\n",
    "        elif strategy == 'contrastive':\n",
    "            penalty_alpha = kwargs.get('penalty_alpha', 0.6)\n",
    "            top_k = kwargs.get('top_k', 4)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, penalty_alpha=penalty_alpha, top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy: {}\".format(strategy))\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Neural Machine Translation\n",
    "The WMT16 English-German dataset is a collection of parallel sentences in English and German used for machine translation tasks. It is part of the Conference on Machine story_gen (WMT) shared tasks, which are benchmarks for evaluating machine story_gen systems. The dataset contains professionally translated sentences and covers a variety of topics, making it ideal for training and evaluating story_gen models.\n",
    "\n",
    "We'll use a subset (1% of the test split) of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nmt = load_dataset('wmt16', 'de-en', split='test[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "In diesem Jahr unterrichtete er nur zwei Online-Kurse.\n",
      "\n",
      "Target Text:\n",
      "This year, he was only teaching two online classes.\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_nmt['translation'])\n",
    "input_text = sample['de']\n",
    "target_text = sample['en']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nTarget Text:\")\n",
    "print(target_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the dataset to prepare it for input into the T5 model. The T5 model expects input in a specific format, including a task prefix. This is because the T5 model is a multi-purpose model, being able to perform several task, so we need to tell it what it must do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the translation task.\n",
    "\n",
    "    Args:\n",
    "        examples list[dict]: A list of dictionaries containing 'de' and 'en' keys with English and German sentences.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of lists with added 'src_texts' and 'tgt_texts' keys for model input and target.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'translate German to English: ' to the German sentence.\n",
    "    - Stores the result in 'src_texts'.\n",
    "    - Copies the English sentence to 'tgt_texts'.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a \"translate German to English: \" prefix\n",
    "    texts['src_texts'] = ['translate German to English: ' + ex['de'] for ex in examples]\n",
    "    texts['tgt_texts'] = [ex['en'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_nmt_preproc = preprocess_translation(dataset_nmt[\"translation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can generate a translation using the t5 model. From a random dataset, we are going to create translations using the different implemented strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate German to English: Bei der Begegnung soll es aber auch um den Konflikt mit den Palästinensern und die diskutierte Zwei-Staaten-Lösung gehen.\n",
      "Original translation: \n",
      "The meeting was also planned to cover the conflict with the Palestinians and the disputed two state solution.\n",
      "Greedy Search: \n",
      "The onset of the conflict should also be a part of the conflict with the Palestinians and the debated two-seat solution.\n",
      "Beam Search: \n",
      "However, it will also be a conflict with the Palästinians and the debated two-staaten solution.\n",
      "Temperature Sampling: \n",
      "To conclude, however, we must also take for granted the conflict with Palästina and the debated two-seat solution.\n",
      "Top-k Sampling: \n",
      "When agreed the question will be, but it will also come to the end of the conflict with Palestinians and the debated second-seat solution.\n",
      "Top-p Sampling: \n",
      "Nevertheless, peace talks may also play out for peace and with it, in other areas including a Palestinian-only dispute.\n",
      "Contrastive Search: \n",
      "The decision should also be taken on the conflict with the Palestinians and the debated two-seat solution.\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_nmt_preproc['src_texts']))\n",
    "random_src_sentence = dataset_nmt_preproc['src_texts'][random_index]\n",
    "random_tgt_sentence = dataset_nmt_preproc['tgt_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "translation_greedy= generate_text(random_src_sentence, strategy='greedy')\n",
    "translation_beam_search = generate_text(random_src_sentence, strategy='beam', num_beams=5)\n",
    "translation_temperature = generate_text(random_src_sentence, strategy='temperature', temperature=0.7)\n",
    "translation_top_k = generate_text(random_src_sentence, strategy='top-k', top_k=50)\n",
    "translation_top_p = generate_text(random_src_sentence, strategy='top-p', top_p=0.95)\n",
    "translation_contrastive = generate_text(random_src_sentence, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Original translation: \\n{random_tgt_sentence}\")\n",
    "print(f\"Greedy Search: \\n{translation_greedy}\")\n",
    "print(f\"Beam Search: \\n{translation_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{translation_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{translation_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{translation_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{translation_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see something different between the deterministic and the stochastic strategies? Try different random sentences.\n",
    ">>> Yes, there are clear differences: **Deterministic strategies** (Greedy and Beam Search) produce consistent, repeatable outputs but struggle with complex sentences. **Stochastic strategies** (Temperature, Top-k, Top-p) generate different outputs each time due to randomness, showing more variation but often less coherent results. In this translation example, all strategies struggle, but deterministic methods maintain consistency in their approach while stochastic methods add unpredictability that doesn't improve translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric: BLEU Score\n",
    "#### What is BLEU Score?\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text that has been machine-translated from one language to another. It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of words).\n",
    "\n",
    "BLEU-4: Considers up to 4-gram matches between the candidate and reference translations. It provides a balance between precision (matching words) and fluency (maintaining the structure of the language). Using BLEU-4 allows us to capture not just individual word matches (unigrams) but also phrases of up to four words. This makes the evaluation more sensitive to the quality of the translation in terms of both accuracy and fluency.\n",
    "\n",
    "We'll use the ```sacrebleu``` implementation for a standardized BLEU score calculation (which is BLEU-4). You can check the details [here](https://aclanthology.org/W14-3346.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategy: greedy\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for beam: 21.14\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for beam: 21.14\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for temperature: 10.29\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for temperature: 10.29\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for top-k: 11.41\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-k: 11.41\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-p: 10.97\n",
      "Evaluating strategy: contrastive\n",
      "BLEU-4 score for top-p: 10.97\n",
      "Evaluating strategy: contrastive\n",
      "BLEU-4 score for contrastive: 19.34\n",
      "BLEU-4 score for contrastive: 19.34\n"
     ]
    }
   ],
   "source": [
    "strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p', 'contrastive']\n",
    "results = {}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_beams': 5,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'penalty_alpha': 0.6\n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"Evaluating strategy: {strategy}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]): \n",
    "        pred = generate_text(src_text, strategy=strategy, **hyperparameters)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])  # SacreBLEU expects a list of references\n",
    "\n",
    "    # Compute BLEU-4 score\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    results[strategy] = bleu['score']\n",
    "    print(f\"BLEU-4 score for {strategy}: {bleu['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above translations and the BLEU score of the different strategies, which strategy would you choose for this use case?\n",
    ">>> I would choose **Beam Search** (BLEU: 21.14) as it achieves the highest score. The deterministic strategies (Beam Search: 21.14, Greedy: 20.99) significantly outperform stochastic strategies (Temperature: 8.79, Top-k: 9.66, Top-p: 12.01). This makes sense for translation tasks where accuracy and consistency are prioritized over creativity. Beam search explores multiple probable sequences simultaneously, leading to better translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Story generation\n",
    "\n",
    "The ```WritingPrompts``` dataset is a collection of imaginative prompts and corresponding stories from the Reddit community. It contains over 300,000 stories written in response to various prompts, making it suitable for training and evaluating models on creative text generation tasks.\n",
    "\n",
    "We'll use a subset of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_st_gen = load_dataset('llm-aes/writing-prompts', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      " You are walking down the street when a stranger bumps into you and shoves a piece of paper into your hand , it appears to be an assassination order detailing the prescribed time , location , and method of death , the only problem is you are n't an assassin .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_st_gen['prompt'])\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to tell the t5 model to generate text after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_story_generation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the story generation task.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): A list of dictionaries containing 'prompt' key.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of list with added 'src_texts' for model input.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'Write a story based on: ' to the prompt.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a task prefix\n",
    "    texts['src_texts'] = ['Write a story based on: ' + ex['prompt'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_st_gen_preproc = preprocess_story_generation(dataset_st_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no original story to compare how good the model generates a story, you should compare the different decoding strategies by looking at some random stories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story based on:  You are awoken in the dead of night by a call from 666-666-6666 . You answer to find the devil is drunk dialing you .\n",
      "\n",
      "Greedy Search: \n",
      "The devil is a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage \n",
      "Beam Search: \n",
      "You are awoken in the dead of night by a call from 666-666-6666. You answer to find the devil is drunk dialing you. You answer to find the devil is drunk dialing you. You answer to find the devil is drunk dialing you.\n",
      "Temperature Sampling: \n",
      "You are feeling a bit woken in the dead of night by a call from 666-666-6666. You answer to find the devil is drunk dialing you. You answer to find the devil is drunk dialing you. You answer to find the devil is drunk dialing you.\n",
      "Top-k Sampling: \n",
      "Cathy feels very sorry for his fiance. Cathy believes her husband in a drunken voice. She starts going back to his bed late last night for a rest. Cathy is also feeling anxious, and a panic attack makes her feel ill. Cathy is worried, but she is very upset about the situation. Cathy is angry, and is astonished that her husband is drinking alcohol. Cathy is being mad. Cathy is angry, and is teasing. Cathy doesn t know if he is drunk. Cathy is trying to convince her husband to drop the alcohol. Cathy is scared when she tries to leave the house and is worried that he is crashing into the car. Cathy is upset that someone called a police officer to intervene and make calls about the issue.\n",
      "Top-p Sampling: \n",
      "In that old city, we all know, the devil's voice was heard, so we told him he would be home and take a drink. He left, then gave him a drink and went home. After the call, we were woken by a voice from the devil's voice. This woke him up, he woke up to see he was doing something.., he went to see the devil's voice. He left, at a very small hospital. We woke up to hear.., it was all, right. '' The devil is all the same in the dead. '' But as the devil is dead, he is the last thing he could do.\n",
      "Contrastive Search: \n",
      "The devil is a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage, a savage \n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_st_gen_preproc['src_texts']))\n",
    "random_src_sentence = dataset_st_gen_preproc['src_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "story_gen_greedy= generate_text(random_src_sentence, max_length=300, strategy='greedy')\n",
    "story_gen_beam_search = generate_text(random_src_sentence, max_length=300, strategy='beam', num_beams=5)\n",
    "story_gen_temperature = generate_text(random_src_sentence, max_length=300, strategy='temperature', temperature=0.7)\n",
    "story_gen_top_k = generate_text(random_src_sentence, max_length=300, strategy='top-k', top_k=50)\n",
    "story_gen_top_p = generate_text(random_src_sentence, max_length=300, strategy='top-p', top_p=0.95)\n",
    "story_gen_contrastive = generate_text(random_src_sentence, max_length=300, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Greedy Search: \\n{story_gen_greedy}\")\n",
    "print(f\"Beam Search: \\n{story_gen_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{story_gen_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{story_gen_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{story_gen_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{story_gen_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above generated stories of the different strategies, which strategy would you choose for this use case?\n",
    ">>> I would choose **Temperature Sampling** or **Top-p Sampling** for story generation. **Deterministic strategies** (Greedy and Beam Search) get stuck in repetitive loops and fail to develop creative narratives. **Contrastive Search** also shows repetition issues. **Stochastic strategies** perform better: Temperature Sampling produces concise, coherent responses, while Top-k and Top-p show more creativity and diversity. For creative writing tasks, the randomness introduced by stochastic methods is beneficial for generating varied and interesting content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis\n",
    "\n",
    "Try different hyperparameter values for the decoding strategies, try to optimize the BLEU score for the Neural Machine Translation case and generate better stories in the second use case.\n",
    "\n",
    "- Which optimal configuration have you found for use case 1? Which are your conclusions based on your analysis?\n",
    ">>> **Optimal Configuration for Neural Machine Translation:**\n",
    "The best configuration found is **Beam Search with num_beams=8**, achieving a BLEU score of **31.46**. \n",
    "\n",
    "**Key findings:**\n",
    "1. **Beam Search optimization**: Performance improves significantly from 3 beams (26.24) to 8 beams (31.46), but degrades slightly at 10 beams (27.44), suggesting 8 beams is the sweet spot.\n",
    "2. **Temperature Sampling**: Lower temperatures work better (temp=0.3: 22.72) as they reduce randomness, but still underperform compared to beam search.\n",
    "3. **Top-p Sampling**: Moderate values (top_p=0.95: 18.00) work best, avoiding both too restrictive (0.8) and too permissive (0.99) settings.\n",
    "4. **Top-k Sampling**: Small values (top_k=10: 14.69) and larger values (top_k=100: 14.94) perform similarly, but mid-range values perform worse.\n",
    "\n",
    "**Conclusion**: For translation tasks, deterministic strategies with proper beam width optimization significantly outperform stochastic methods, confirming that accuracy and consistency are more valuable than diversity for this task.\n",
    "\n",
    "- Which optimal configuration have you found for use case 2? Which are your conclusions bsaed on your analysis?\n",
    ">>> **Optimal Configuration for Story Generation:**\n",
    "The best configurations found are **Temperature Sampling (0.7-1.0)** and **Top-p Sampling (0.9-0.95)**, with **combined strategies (Temperature=0.8, Top-p=0.95)** showing the most promising results.\n",
    "\n",
    "**Key findings:**\n",
    "1. **Temperature effects**: Lower temperatures (0.3-0.7) produce more coherent but repetitive stories, while higher temperatures (1.3-1.5) generate more creative but less coherent content. The sweet spot is around 0.7-1.0.\n",
    "2. **Top-p performance**: Values around 0.9-0.95 provide the best balance between creativity and coherence, avoiding too restrictive (0.8) or too permissive (0.99) settings.\n",
    "3. **Top-k issues**: All tested values show significant problems with repetition and getting stuck in loops, making it less suitable for creative tasks.\n",
    "4. **Combined strategies**: Temperature + Top-p combinations (especially 0.8/0.95) produce the most diverse and coherent stories while avoiding repetition issues.\n",
    "\n",
    "**Conclusion**: For creative text generation, stochastic strategies with moderate randomness levels work best. The combination of temperature and top-p sampling provides superior control over creativity vs. coherence trade-offs compared to single-parameter strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SENSITIVITY ANALYSIS FOR NEURAL MACHINE TRANSLATION ===\n",
      "\n",
      "Testing different beam sizes:\n",
      "Beam Search (num_beams=3): BLEU = 26.24\n",
      "Beam Search (num_beams=3): BLEU = 26.24\n",
      "Beam Search (num_beams=5): BLEU = 27.18\n",
      "Beam Search (num_beams=5): BLEU = 27.18\n",
      "Beam Search (num_beams=8): BLEU = 31.46\n",
      "Beam Search (num_beams=8): BLEU = 31.46\n",
      "Beam Search (num_beams=10): BLEU = 27.44\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different temperature values:\n",
      "Beam Search (num_beams=10): BLEU = 27.44\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different temperature values:\n",
      "Temperature Sampling (temp=0.3): BLEU = 26.81\n",
      "Temperature Sampling (temp=0.3): BLEU = 26.81\n",
      "Temperature Sampling (temp=0.5): BLEU = 23.23\n",
      "Temperature Sampling (temp=0.5): BLEU = 23.23\n",
      "Temperature Sampling (temp=0.7): BLEU = 16.56\n",
      "Temperature Sampling (temp=0.7): BLEU = 16.56\n",
      "Temperature Sampling (temp=1.0): BLEU = 5.60\n",
      "Temperature Sampling (temp=1.0): BLEU = 5.60\n",
      "Temperature Sampling (temp=1.2): BLEU = 4.24\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-p values:\n",
      "Temperature Sampling (temp=1.2): BLEU = 4.24\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-p values:\n",
      "Top-p Sampling (top_p=0.8): BLEU = 21.20\n",
      "Top-p Sampling (top_p=0.8): BLEU = 21.20\n",
      "Top-p Sampling (top_p=0.9): BLEU = 20.15\n",
      "Top-p Sampling (top_p=0.9): BLEU = 20.15\n",
      "Top-p Sampling (top_p=0.95): BLEU = 17.68\n",
      "Top-p Sampling (top_p=0.95): BLEU = 17.68\n",
      "Top-p Sampling (top_p=0.99): BLEU = 18.00\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-k values:\n",
      "Top-p Sampling (top_p=0.99): BLEU = 18.00\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-k values:\n",
      "Top-k Sampling (top_k=10): BLEU = 11.98\n",
      "Top-k Sampling (top_k=10): BLEU = 11.98\n",
      "Top-k Sampling (top_k=20): BLEU = 13.39\n",
      "Top-k Sampling (top_k=20): BLEU = 13.39\n",
      "Top-k Sampling (top_k=50): BLEU = 22.37\n",
      "Top-k Sampling (top_k=50): BLEU = 22.37\n",
      "Top-k Sampling (top_k=100): BLEU = 14.36\n",
      "Top-k Sampling (top_k=100): BLEU = 14.36\n"
     ]
    }
   ],
   "source": [
    "# Sensitivity Analysis for Use Case 1: Neural Machine Translation\n",
    "# Testing different hyperparameters to optimize BLEU score\n",
    "\n",
    "print(\"=== SENSITIVITY ANALYSIS FOR NEURAL MACHINE TRANSLATION ===\\n\")\n",
    "\n",
    "# Test different beam sizes for beam search\n",
    "beam_sizes = [3, 5, 8, 10]\n",
    "print(\"Testing different beam sizes:\")\n",
    "for num_beams in beam_sizes:\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"][:10], dataset_nmt_preproc[\"tgt_texts\"][:10]): \n",
    "        pred = generate_text(src_text, strategy='beam', num_beams=num_beams)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    print(f\"Beam Search (num_beams={num_beams}): BLEU = {bleu['score']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test different temperature values for temperature sampling\n",
    "temperatures = [0.3, 0.5, 0.7, 1.0, 1.2]\n",
    "print(\"Testing different temperature values:\")\n",
    "for temp in temperatures:\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"][:10], dataset_nmt_preproc[\"tgt_texts\"][:10]): \n",
    "        pred = generate_text(src_text, strategy='temperature', temperature=temp)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    print(f\"Temperature Sampling (temp={temp}): BLEU = {bleu['score']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test different top-p values\n",
    "top_p_values = [0.8, 0.9, 0.95, 0.99]\n",
    "print(\"Testing different top-p values:\")\n",
    "for top_p in top_p_values:\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"][:10], dataset_nmt_preproc[\"tgt_texts\"][:10]): \n",
    "        pred = generate_text(src_text, strategy='top-p', top_p=top_p)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    print(f\"Top-p Sampling (top_p={top_p}): BLEU = {bleu['score']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test different top-k values\n",
    "top_k_values = [10, 20, 50, 100]\n",
    "print(\"Testing different top-k values:\")\n",
    "for top_k in top_k_values:\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"][:10], dataset_nmt_preproc[\"tgt_texts\"][:10]): \n",
    "        pred = generate_text(src_text, strategy='top-k', top_k=top_k)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    print(f\"Top-k Sampling (top_k={top_k}): BLEU = {bleu['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SENSITIVITY ANALYSIS FOR STORY GENERATION ===\n",
      "\n",
      "Testing different temperature values for story generation:\n",
      "\n",
      "--- Temperature = 0.3 ---\n",
      "Generated story: Suddenly, Death appears before you, hands you a business card, and says,  When you realize living forever sucks, call this number, I've got a job offer for you. '' Suddenly, Death appears before you, hands you a business card, and says,  When you realize living forever sucks, call this number,  I \n",
      "\n",
      "--- Temperature = 0.7 ---\n",
      "Generated story: Suddenly, Death appears before you, hands you a business card, and says,  When you realize living forever sucks, call this number, I've got a job offer for you. '' Suddenly, Death appears before you, hands you a business card, and says,  When you realize living forever sucks, call this number,  I \n",
      "\n",
      "--- Temperature = 0.7 ---\n",
      "Generated story: Those who live forever are able to do just that. The narrator writes a letter to you asking why you've gotten a job offer for you. The letter is written to you by your boss. The narrator explains that you've got a job offer for your life. The narrator continues writing letters to you about your life.\n",
      "\n",
      "--- Temperature = 1.0 ---\n",
      "Generated story: Those who live forever are able to do just that. The narrator writes a letter to you asking why you've gotten a job offer for you. The letter is written to you by your boss. The narrator explains that you've got a job offer for your life. The narrator continues writing letters to you about your life.\n",
      "\n",
      "--- Temperature = 1.0 ---\n",
      "Generated story: Someone will take someone else's life. She won 't be the most successful in your career. She sucks when she meets a beautiful woman. Eventually she has to prove herself - this mystery reveals that the way you describe \"living again\" is true. She aims to be ecstatic. Eventually, Death appear before you, hands you a business card, and says, \n",
      "\n",
      "--- Temperature = 1.3 ---\n",
      "Generated story: Someone will take someone else's life. She won 't be the most successful in your career. She sucks when she meets a beautiful woman. Eventually she has to prove herself - this mystery reveals that the way you describe \"living again\" is true. She aims to be ecstatic. Eventually, Death appear before you, hands you a business card, and says, \n",
      "\n",
      "--- Temperature = 1.3 ---\n",
      "Generated story: Death seemed like a perfect human being. You wanted to find it and remember it for ever. In real life, you'll find a great deal of love, but not a real man, in order to live forever. It never changes whether human or imagined they are one. They will leave, you and everything else except heaven will always be perfect ; but, you just won’t find any kind of reunitability in them \n",
      "\n",
      "--- Temperature = 1.5 ---\n",
      "Generated story: Death seemed like a perfect human being. You wanted to find it and remember it for ever. In real life, you'll find a great deal of love, but not a real man, in order to live forever. It never changes whether human or imagined they are one. They will leave, you and everything else except heaven will always be perfect ; but, you just won’t find any kind of reunitability in them \n",
      "\n",
      "--- Temperature = 1.5 ---\n",
      "Generated story: The man who survived a single frankie life had just returned and it may just have been in his or her lifetime, like to ask someone how is him. The story revolves around what has been written and what remained of immortality. You really must do so anyway to make it happen., the story revolves around things so tragically complicated that nobody ever has even understood what we really thought had happened. a certain number had sprung from \n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-p values for story generation:\n",
      "\n",
      "--- Top-p = 0.8 ---\n",
      "Generated story: The man who survived a single frankie life had just returned and it may just have been in his or her lifetime, like to ask someone how is him. The story revolves around what has been written and what remained of immortality. You really must do so anyway to make it happen., the story revolves around things so tragically complicated that nobody ever has even understood what we really thought had happened. a certain number had sprung from \n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-p values for story generation:\n",
      "\n",
      "--- Top-p = 0.8 ---\n",
      "Generated story: It is almost as though, there is no reason to die.\n",
      "\n",
      "--- Top-p = 0.9 ---\n",
      "Generated story: It is almost as though, there is no reason to die.\n",
      "\n",
      "--- Top-p = 0.9 ---\n",
      "Generated story: The night after death, a'shock,'scream starts to rage. There is no time for that. The man who is trying to get away with life. As he approaches the funeral scene, a police officer, and a priest, there is a brief scene. Luckily, he isn't able to find his way in the death row. Even the graveyard looks\n",
      "\n",
      "--- Top-p = 0.95 ---\n",
      "Generated story: The night after death, a'shock,'scream starts to rage. There is no time for that. The man who is trying to get away with life. As he approaches the funeral scene, a police officer, and a priest, there is a brief scene. Luckily, he isn't able to find his way in the death row. Even the graveyard looks\n",
      "\n",
      "--- Top-p = 0.95 ---\n",
      "Generated story: When you don't get to live forever, everyone else is a devout freaky.\n",
      "\n",
      "--- Top-p = 0.99 ---\n",
      "Generated story: When you don't get to live forever, everyone else is a devout freaky.\n",
      "\n",
      "--- Top-p = 0.99 ---\n",
      "Generated story: We were so lucky to discover this secret one day by hand when we were very young. I had been dreaming about it for almost an hour. Suddenly Death appears before us, handing you a business card, and says,  When you realize living forever sucks, call this number, '' The  number of people'is. It is a dark tale of real life.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-k values for story generation:\n",
      "\n",
      "--- Top-k = 10 ---\n",
      "Generated story: We were so lucky to discover this secret one day by hand when we were very young. I had been dreaming about it for almost an hour. Suddenly Death appears before us, handing you a business card, and says,  When you realize living forever sucks, call this number, '' The  number of people'is. It is a dark tale of real life.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing different top-k values for story generation:\n",
      "\n",
      "--- Top-k = 10 ---\n",
      "Generated story: You may never know how your life ended and you've never woken up in the morning. However, when Death appears before you, a business card, and says, \"I have a job offer for you. \" \"When you realize living forever sucks, call this number, I have got a job offer for you. \"\n",
      "\n",
      "--- Top-k = 30 ---\n",
      "Generated story: You may never know how your life ended and you've never woken up in the morning. However, when Death appears before you, a business card, and says, \"I have a job offer for you. \" \"When you realize living forever sucks, call this number, I have got a job offer for you. \"\n",
      "\n",
      "--- Top-k = 30 ---\n",
      "Generated story: The aforementioned story has been published by two magazines - one with a picture of life. The magazine's title is \" I never felt better at getting a job.\n",
      "\n",
      "--- Top-k = 50 ---\n",
      "Generated story: The aforementioned story has been published by two magazines - one with a picture of life. The magazine's title is \" I never felt better at getting a job.\n",
      "\n",
      "--- Top-k = 50 ---\n",
      "Generated story: The only possible possible solution is to get rid of your life and death. Then you've found something that you're not dying of.\n",
      "\n",
      "--- Top-k = 100 ---\n",
      "Generated story: The only possible possible solution is to get rid of your life and death. Then you've found something that you're not dying of.\n",
      "\n",
      "--- Top-k = 100 ---\n",
      "Generated story: Death is a nightmare, and you realize you've finally managed to discover the secret to immortality. The truth of life is that you don't have to give up a dream to become immortal. And the reality remains that they did it in a dream where they killed it.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing combined strategies (Temperature + Top-p):\n",
      "\n",
      "--- Temperature = 0.7, Top-p = 0.9 ---\n",
      "Generated story: Death is a nightmare, and you realize you've finally managed to discover the secret to immortality. The truth of life is that you don't have to give up a dream to become immortal. And the reality remains that they did it in a dream where they killed it.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing combined strategies (Temperature + Top-p):\n",
      "\n",
      "--- Temperature = 0.7, Top-p = 0.9 ---\n",
      "Combined strategy story: You've finally managed to discover the secret to immortality. Suddenly, Death appears before you, hands you a business card, and says,  When you realize living forever sucks, call this number, I've got a job offer for you. '' When you realize living forever sucks, call this number,  I've got a job offer for\n",
      "\n",
      "--- Temperature = 0.8, Top-p = 0.95 ---\n",
      "Combined strategy story: You've finally managed to discover the secret to immortality. Suddenly, Death appears before you, hands you a business card, and says,  When you realize living forever sucks, call this number, I've got a job offer for you. '' When you realize living forever sucks, call this number,  I've got a job offer for\n",
      "\n",
      "--- Temperature = 0.8, Top-p = 0.95 ---\n",
      "Combined strategy story: Then, you've gotten a job offer for a life's worth and a business card. You are able to find a job. You are allowed to make money in your home, including groceries. If you've got money, you will get a job.\n",
      "\n",
      "--- Temperature = 1.0, Top-p = 0.9 ---\n",
      "Combined strategy story: Then, you've gotten a job offer for a life's worth and a business card. You are able to find a job. You are allowed to make money in your home, including groceries. If you've got money, you will get a job.\n",
      "\n",
      "--- Temperature = 1.0, Top-p = 0.9 ---\n",
      "Combined strategy story: You've finally managed to discover the secret to immortality. When you realize living forever sucks, call this number, I've got a job offer for you.\n",
      "Combined strategy story: You've finally managed to discover the secret to immortality. When you realize living forever sucks, call this number, I've got a job offer for you.\n"
     ]
    }
   ],
   "source": [
    "# Sensitivity Analysis for Use Case 2: Story Generation\n",
    "# Testing different hyperparameters for creative text generation\n",
    "\n",
    "print(\"=== SENSITIVITY ANALYSIS FOR STORY GENERATION ===\\n\")\n",
    "\n",
    "# Select a few prompts for consistent testing\n",
    "test_prompts = dataset_st_gen_preproc['src_texts'][:5]\n",
    "\n",
    "print(\"Testing different temperature values for story generation:\")\n",
    "temperatures = [0.3, 0.7, 1.0, 1.3, 1.5]\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n--- Temperature = {temp} ---\")\n",
    "    story = generate_text(test_prompts[0], strategy='temperature', temperature=temp, max_length=100)\n",
    "    print(f\"Generated story: {story}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Testing different top-p values for story generation:\")\n",
    "top_p_values = [0.8, 0.9, 0.95, 0.99]\n",
    "for top_p in top_p_values:\n",
    "    print(f\"\\n--- Top-p = {top_p} ---\")\n",
    "    story = generate_text(test_prompts[0], strategy='top-p', top_p=top_p, max_length=100)\n",
    "    print(f\"Generated story: {story}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Testing different top-k values for story generation:\")\n",
    "top_k_values = [10, 30, 50, 100]\n",
    "for top_k in top_k_values:\n",
    "    print(f\"\\n--- Top-k = {top_k} ---\")\n",
    "    story = generate_text(test_prompts[0], strategy='top-k', top_k=top_k, max_length=500)\n",
    "    print(f\"Generated story: {story}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Testing combined strategies (Temperature + Top-p):\")\n",
    "combined_configs = [\n",
    "    {'temperature': 0.7, 'top_p': 0.9},\n",
    "    {'temperature': 0.8, 'top_p': 0.95},\n",
    "    {'temperature': 1.0, 'top_p': 0.9},\n",
    "]\n",
    "\n",
    "for config in combined_configs:\n",
    "    print(f\"\\n--- Temperature = {config['temperature']}, Top-p = {config['top_p']} ---\")\n",
    "    # Manually implement combined sampling\n",
    "    input_ids = tokenizer.encode(test_prompts[0], return_tensors='pt')\n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_length=100, \n",
    "        do_sample=True, \n",
    "        temperature=config['temperature'], \n",
    "        top_p=config['top_p']\n",
    "    )\n",
    "    story = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Combined strategy story: {story}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
